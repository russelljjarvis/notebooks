{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/how-to-use-pytorch-as-a-general-optimizer-a91cbf72a7fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from copy import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from jithub.models import model_classes\n",
    "model = model_classes.ADEXPModel()\n",
    "from neuronunit.optimization.model_parameters import MODEL_PARAMS\n",
    "from neuronunit.tests.target_spike_current import SpikeCountSearch\n",
    "\n",
    "import quantities as qt\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "n = 1000\n",
    "noise = torch.Tensor(np.random.normal(0, 0.02, size=n))\n",
    "x = torch.arange(n)\n",
    "\n",
    "##\n",
    "# simulate a ground truth neuron here.\n",
    "##\n",
    "\n",
    "a, k, b = 0.7, .01, 0.2\n",
    "ground_truth = a * np.exp(-k * x) + b# + noise\n",
    "print(np.shape(ground_truth))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"Custom Pytorch model for gradient optimization.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        weights = torch.distributions.Uniform(0.1, 1.0).sample((10,))\n",
    "        self.weights = nn.Parameter(weights)        \n",
    "        self.nspikes = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Implement function to be optimised. In this case, an exponential decay\n",
    "        function (a + exp(-k * X) + b),\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        params = {}\n",
    "        i = 0\n",
    "        for k,j in MODEL_PARAMS['ADEXP'].items():\n",
    "            wv = self.weights.detach().numpy()[0]\n",
    "            params[k] = np.mean([wv*j[0],wv*j[1]])\n",
    "            i += 1\n",
    "        model = model_classes.ADEXPModel()\n",
    "        model.attrs = params\n",
    "        \n",
    "        observation = {}\n",
    "        observation[\"value\"] = self.nspikes\n",
    "        scs = SpikeCountSearch(observation)\n",
    "        target_current = scs.generate_prediction(model)\n",
    "        inject_param = {\n",
    "            \"padding\":0 * qt.ms,\n",
    "            \"delay\": 0 * qt.ms,\n",
    "            \"amplitude\": 900 * qt.pA,\n",
    "            \"duration\": 1000 * qt.ms,\n",
    "            \"dt\":0.25\n",
    "        }\n",
    "        try:\n",
    "            inject_param[\"amplitude\"] = target_current[\"value\"]* qt.pA\n",
    "            model.inject_square_current(**inject_param)\n",
    "            vm = model.get_membrane_potential()\n",
    "            vm_=[v[0] for v in vm]\n",
    "            #plt.plot(vm.times,vm)\n",
    "            #plt.show()\n",
    "\n",
    "            #concise_spk_time = model.get_spike_train()\n",
    "        except:\n",
    "            # some model parameters will cause try block to fail.\n",
    "            #concise_spk_time = []\n",
    "            vm = 1000.0\n",
    "        #return concise_spk_time\n",
    "        #a, k, b = self.weights\n",
    "        print(vm)\n",
    "        n = 1000\n",
    "#noise = torch.Tensor(np.random.normal(0, 0.02, size=n))\n",
    "        x = torch.arange(n)\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        return torch.as_tensor(vm_)#concise_spk_time#a * torch.exp(-k * X) + b\n",
    "    \n",
    "def training_loop(model, optimizer, n=1000):\n",
    "    \"Training loop for torch model.\"\n",
    "    losses = []\n",
    "    for i in range(n):\n",
    "        preds = model(x)\n",
    "        print(np.shape(preds),np.shape(ground_truth))\n",
    "        model.nspikes=10\n",
    "        print(preds.clone().detach().numpy())\n",
    "        #print(ground_truth)\n",
    "        loss = F.mse_loss(preds, ground_truth).sqrt()\n",
    "        print(loss)\n",
    "        #print(optimizer.requires_grad)\n",
    "        print(loss.requires_grad)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        losses.append(loss)  \n",
    "    return losses\n",
    "\"\"\"\n",
    "If you are familiar with Pytorch there is nothing too fancy going on here. The key thing that we are doing here is defining our own weights and manually registering these as Pytorch parameters — that is what these lines do:\n",
    "weights = torch.distributions.Uniform(0, 0.1).sample((3,))\n",
    "# make weights torch parameters\n",
    "self.weights = nn.Parameter(weights)\n",
    "The lines below detemine the function to be optimised. You can replace these with the definition of the function you want to minimise.\n",
    "a, k, b = self.weights\n",
    "return a * torch.exp(-k * X) + b\n",
    "By calling nn.Parameter the weight we define will behave and function in the same way as standard Pytorch parameters — i.e they can calculate gradients and be updated in response to a loss function. The training loop is simply iterating over n epochs, each time estimating the mean squared error and updating the gradients.\n",
    "Time to run the model, we’ll use Adam for the optimization.\n",
    "# instantiate model\n",
    "\"\"\"\n",
    "m = Model()\n",
    "m.n_spikes=10\n",
    "# Instantiate optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-15.23517741]\n",
      " [-15.23594829]\n",
      " [-15.23670814]\n",
      " ...\n",
      " [-15.23069351]\n",
      " [-15.23069246]\n",
      " [-15.23069141]] mV\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f11c7b41fd0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a746b4e3b56e>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, optimizer, n)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnspikes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a746b4e3b56e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#import pdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvm_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#concise_spk_time#a * torch.exp(-k * X) + b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "losses = training_loop(m, opt)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(losses)\n",
    "print(m.weights)\n",
    "\n",
    "#Losses over 1000 epochs — Image by Author..\n",
    "#The plot above shows the loss function over 1000 epochs — you can see that after ~600 it is showing no signs of further improvement. The estimated weights for a, k, b are 0.697, 0.0099, 0.1996, so extremely close to the parameters that define the function and we can use the trained model to estimate the function:\n",
    "preds = m(x)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.scatter(x, preds.detach().numpy())\n",
    "plt.scatter(x, y, alpha=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(vm_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
